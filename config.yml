model: lstm
epochs: 2
learning_rate: 1e-3
optimizer: adam
sequence_length: 100
embedding_size: 200
hidden_size: [256]
batch_size: 128
vocab_size: 10000
num_classes: 1
train_data_path: dataset/train.txt
eval_data_path: dataset/eval.txt
test_data_path: dataset/test.txt
stopwords_path: dataset/stopwords.txt
output_path: outputs/